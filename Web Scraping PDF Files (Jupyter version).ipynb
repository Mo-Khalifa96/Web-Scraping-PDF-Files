{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e9b214",
   "metadata": {},
   "source": [
    "### Web Scraping PDF Files\n",
    "<br>\n",
    "\n",
    "**The following code is a simple program for accessing the web and extracting links to PDF files from any given web page (provided it references links to PDF files anywhere on the page). I have include three versions of the code: the first simply searches for and extracts any pdf links to be found, the second applies filters to ensure these links are valid and active, and the third makes the code more generally applicable, allowing the user to copy and paste any web page they like (instead of using only one sample web pages as in versions 1 and 2).**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Note, before running any part of the program, the first two cells must be executed first. The first cell installs the necessary Python libraries whilst the second imports them, making them ready for use. You can run each version of the program individually by selecting its respective cell and clicking on the 'Run' icon. As mentioned, for the third version of the program will prompt you to enter any given web address you choose, and so you can run and re-run it multiple times to try scraping different web pages, provided they contain links to pdf files. Feel free to test the program in whatever way you like.\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89946ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the Python modules to be used \n",
    "!pip install requests \n",
    "!pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c0be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Python modules to use \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630f38e",
   "metadata": {},
   "source": [
    "### Version 1:\n",
    "<br> \n",
    "For this version, I will simply search for and extract PDF links from the wikipedia page on 'Memory'. As such, I will utilize the requests module to access the web page and the beautiful soup library to parse the page and identify the links. \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e042cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link: https://scholar.harvard.edu/files/schacterlab/files/grafschacter1985.pdf\n",
      "link: http://www.saylor.org/site/wp-content/uploads/2011/01/TLBrink_PSYCH07.pdf\n",
      "link: http://bernard.pitzer.edu/~dmoore/psych199s03articles/R-Collier_memory.pdf\n",
      "link: http://www.crossingdialogues.com/Ms-A14-03.pdf\n",
      "link: https://web.archive.org/web/20070719053600/http://www.ilcusa.org/_lib/pdf/ISOA.pdf\n",
      "link: http://www.ilcusa.org/_lib/pdf/isoa.pdf\n"
     ]
    }
   ],
   "source": [
    "#First, specifying the url (web address) to the wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Memory\"\n",
    "\n",
    "#making a get request to access web page\n",
    "page = requests.get(url)\n",
    "\n",
    "#extracting HTML document\n",
    "html = page.text\n",
    "\n",
    "#creating soup object to parse the HTML document\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#to retreive the pdf links (using a regular expression to match with)\n",
    "pdf_links = soup.find_all(href=re.compile('.+\\.pdf'))        #only matches values that end in '.pdf'\n",
    "\n",
    "#to display the links\n",
    "for pdf in pdf_links:\n",
    "    print('link:', pdf.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec67505",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef8f0d",
   "metadata": {},
   "source": [
    "### Version 2:\n",
    "<br> \n",
    "This version will be similar to before, scraping pdf links from a web page, except this time I will make sure to save only those links that are active into a list, and discard those that are not. To do so, I have include two filters, the first makes sure that the pdf link works at all, i.e., the referenced web page in fact exists, meanwhile the second filter makes sure that the link can be accessed given that sometimes a link might reference a web page that exists and yet is inaccessible (this could be due to access being unauthorized or the requested page is not available, despite being active, among others). Finally, this time I'll access another wikipedia page, the page on 'Attention'. \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b801ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page changed or is not found: https://www.aaafoundation.org/sites/default/files/MeasuringCognitiveDistractions.pdf\n",
      "Page changed or is not found: http://www.princeton.edu/~kahneman/docs/attention_and_effort/Attention_lo_quality.pdf\n",
      "Page does not exist: http://www.psych.utoronto.ca/users/ferber/teaching/visualattention/readings/Oct6/1998_Friesen_Kingstone_PBR.pdf\n",
      "Page does not exist: http://cns-web.bu.edu/Profiles/Mingolla.html/cnsftp/cn730-2007-pdf/posner_petersen90.pdf\n",
      "Page changed or is not found: http://www.icn.ucl.ac.uk/lavielab/reprints/Lavie-etal-04.pdf\n",
      "Page does not exist: http://www.klab.caltech.edu/~xhou/papers/cvpr07.pdf\n",
      "Page changed or is not found: http://www.cim.mcgill.ca/~lijian/06243147.pdf\n",
      "\n",
      "Active link: http://people.ucsc.edu/~brogoff/Psych247articles/Morelli%20et%20al%20Cultural%20Var%20in%20Young%20Children%27s%20Access.pdf\n",
      "Active link: https://web.archive.org/web/20130626052615/http://www.icn.ucl.ac.uk/lavielab/reprints/lavie-etal-04.pdf\n",
      "Active link: https://www.msu.edu/~ema/802/Ch3-4-SpelkeEtAl75.pdf\n",
      "Active link: https://qilin-zhang.github.io/_pages/pdfs/sensors-18-01979-Action_Recognition_by_an_Attention-Aware_Temporal_Weighted_Convolutional_Neural_Network.pdf\n",
      "Active link: https://web.archive.org/web/20131028233322/https://www.aaafoundation.org/sites/default/files/MeasuringCognitiveDistractions.pdf\n",
      "Active link: https://web.archive.org/web/20130301015810/http://www.cim.mcgill.ca/~lijian/06243147.pdf\n",
      "Active link: https://web.archive.org/web/20150212171627/http://www.klab.caltech.edu/~xhou/papers/cvpr07.pdf\n",
      "Active link: http://psych.unl.edu/mdodd/Psy498/Posner.pdf\n",
      "Active link: https://web.archive.org/web/20150420124015/http://cns-web.bu.edu/Profiles/Mingolla.html/cnsftp/cn730-2007-pdf/posner_petersen90.pdf\n",
      "Active link: http://www-personal.umich.edu/~jjonides/pdf/1983_4.pdf\n",
      "\n",
      "Total number of all links retrieved: 17\n",
      "Total number of active links: 10\n"
     ]
    }
   ],
   "source": [
    "#Specifying the url (web address) to the wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Attention\"\n",
    "\n",
    "#making a get request to access web page\n",
    "page = requests.get(url)\n",
    "\n",
    "#extracting HTML document\n",
    "html = page.text\n",
    "\n",
    "#creating soup object to parse the HTML document\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#creating a set object to save active links into\n",
    "active_links_set = set()\n",
    "\n",
    "\n",
    "#Searching for and retrieving the pdf links\n",
    "pdf_links = soup.find_all(href=re.compile('.+\\.pdf'))       # only matches values that end in '.pdf'\n",
    "\n",
    "\n",
    "#Save only those that are active\n",
    "for element in pdf_links:\n",
    "    pdf = element.get('href')     #extract pdf link \n",
    "    \n",
    "    #Filter 1: checking if the referenced web page exists all \n",
    "    try: \n",
    "        req = requests.get(pdf) \n",
    "    except:\n",
    "        print('Page does not exist:', pdf)\n",
    "        continue \n",
    "        \n",
    "    #Filter 2: checking the status code to make sure the link is accessible    \n",
    "    pdf_status_code = requests.get(pdf).status_code\n",
    "    if pdf_status_code != 200:            # status code != 200 would mean that the web page cannot be accessed (even if the link is active)\n",
    "        print('Page changed or is not found:', pdf)\n",
    "        continue\n",
    "    \n",
    "    #adding active link to the set\n",
    "    active_links_set.add(pdf)\n",
    "\n",
    "print('')\n",
    "\n",
    "#displaying the active links \n",
    "for pdf in active_links_set:\n",
    "    print('Active link:', pdf)\n",
    "print('')\n",
    "\n",
    "#Comparing the number of retrieved pdf links vs. only active links \n",
    "print(\"Total number of all links retrieved:\", len(pdf_links))\n",
    "print('Total number of active links:', len(active_links_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81aa8cc",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182929ee",
   "metadata": {},
   "source": [
    "### Version 3:\n",
    "<br> \n",
    "This version is more general purpose. It consists of the same code as before, except this time it allows the user to enter any web page they like. More coded was also added to ensure the entered web address is valid and/or that it does contain any pdf links. It also filters the pdf links and saves only the active ones. \n",
    "<br> \n",
    "To run this cell, click on the 'Run' icon above. Feel free to try it several times on different web pages to scrape pdf links from.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #Prompting the user for a url (web address)\n",
    "    url = input('Enter web address: ')\n",
    "    \n",
    "    #Checking if the web page entered exists \n",
    "    try:\n",
    "        #making a get request to access web page\n",
    "        page = requests.get(url)\n",
    "        #extracting the HTML document\n",
    "        html = page.text\n",
    "    except:\n",
    "        print('Web Address is incorrect or does not exist. Please try again.')\n",
    "        continue \n",
    "\n",
    "\n",
    "    #Creating soup object to parse the HTML document\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #creating a set object to save active links into\n",
    "    active_links_set = set()\n",
    "\n",
    "\n",
    "    #Searching for & retreiving pdf links (if any are found)\n",
    "    pdf_links = soup.find_all(href=re.compile('.+\\.pdf'))       #only matches values that end in '.pdf'\n",
    "\n",
    "    #checking if any pdf links were found\n",
    "    if len(pdf_links) > 0:\n",
    "        break           #if yes, breaks from the loop\n",
    "    else:\n",
    "        print('Page does not contain any PDF links. Try a different web page.')\n",
    "        continue \n",
    "\n",
    "\n",
    "#Saving only those that are active\n",
    "for element in pdf_links:\n",
    "    pdf = element.get('href')     #extract pdf link \n",
    "    \n",
    "    #Filter 1: checking if the referenced web page exists all \n",
    "    try: \n",
    "        req = requests.get(pdf) \n",
    "    except:\n",
    "        continue \n",
    "        \n",
    "    #Filter 2: checking the status code to make sure the link is accessible    \n",
    "    pdf_status_code = requests.get(pdf).status_code\n",
    "    if pdf_status_code != 200:            # status code != 200 would mean that the web page cannot be accessed (even if the link is active)\n",
    "        continue\n",
    "    \n",
    "    #adding active link to the set\n",
    "    active_links_set.add(pdf)\n",
    "\n",
    "\n",
    "#Displaying the retrieved pdf links \n",
    "for pdf in active_links_set:\n",
    "    print('Link:', pdf)\n",
    "\n",
    "#displaying the number of pdf links retrieved \n",
    "print('\\nTotal number of active pdf links retrieved:', len(active_links_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde9473",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
